{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 06 22:00:52 2017\n",
    "\n",
    "@author: binoy\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from cStringIO import StringIO\n",
    "\n",
    "\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = file(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "\n",
    "    return data\n",
    "    \n",
    "with open('ken.txt','w') as f:\n",
    "    f.write(pdfparser('Ken_resume (2).pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gli26\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from gensim.summarization import keywords\n",
    "import pandas as pd \n",
    "\n",
    "import gensim\n",
    "import logging\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import unicodedata\n",
    "import math\n",
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 06 21:03:42 2017\n",
    "\n",
    "@author: binoy\n",
    "\"\"\"\n",
    "\n",
    "class MyCountVectorizer:\n",
    "    def __init__(self, docs):\n",
    "        self.corpus = self.normalize_corpus(docs)\n",
    "        self.make_features()\n",
    "       # self.make_matrix()\n",
    "        \n",
    "    def normalize_corpus(self, docs):    \n",
    "        table = string.maketrans(string.punctuation, \n",
    "                                 len(string.punctuation) * ' ')\n",
    "        norm_docs = []\n",
    "        for doc_raw in docs:\n",
    "            doc = filter(lambda x: x in string.printable, doc_raw)\n",
    "            '''\n",
    "            doc = ''\n",
    "            for x in doc_raw:\n",
    "                if x in string.printable:\n",
    "                    doc += x\n",
    "            '''\n",
    "            doc = str(doc).translate(table).lower()\n",
    "            norm_docs.append(doc)\n",
    "        #self.corpus = norm_docs\n",
    "        return norm_docs\n",
    "        \n",
    "    def make_features(self):\n",
    "        ''' create vocabulary set from the corpus '''\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.features = set()\n",
    "        for doc in self.corpus:\n",
    "            for word in doc.split():\n",
    "                if word not in stopwords:\n",
    "                    self.features.add(word)\n",
    "        #self.features = set([word for doc in self.corpus for word in doc.split() if word not in stopwords])\n",
    "        self.features = sorted(list(self.features))\n",
    "\n",
    "    def make_matrix(self):\n",
    "        self.matrix = []\n",
    "        for doc in self.corpus:\n",
    "            doc_vec = []\n",
    "            for word in self.features:\n",
    "                tf = self.term_freq(word, doc)\n",
    "                doc_vec.append(tf)\n",
    "            self.matrix.append(doc_vec)\n",
    "\n",
    "    def term_freq(self, term, document):\n",
    "        words = document.split()\n",
    "        count = 0\n",
    "        for word in words:\n",
    "            if word == term:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def print_matrix(self):\n",
    "        for vec in self.matrix:\n",
    "            print(vec)\n",
    "\n",
    "    def get_matrix(self):\n",
    "        return self.matrix\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    \n",
    "    def get_density(self):\n",
    "        ''' get the density (# of non-zero elements / # all elements )'''\n",
    "        counter = 0\n",
    "        total = 0\n",
    "        for row in self.matrix:\n",
    "            for item in row:\n",
    "                if item != 0:\n",
    "                    counter += 1\n",
    "                total += 1\n",
    "        return 1.0 * counter / total\n",
    "        \n",
    "\n",
    "\n",
    "class MyTfIdfVectorizer(MyCountVectorizer):\n",
    "    ''' inherits from MyCountVectorizer'''\n",
    "\n",
    "    def make_matrix(self):\n",
    "        'overriding method'\n",
    "        self.matrix = []\n",
    "        for doc in self.corpus:\n",
    "            doc_vec = []\n",
    "            for word in self.features:\n",
    "                tf = self.term_freq(word, doc)\n",
    "                idf = self.inverse_document_freq(word)\n",
    "                doc_vec.append(tf * idf)\n",
    "            #self.matrix.append(doc_vec)\n",
    "            total = sum(doc_vec)\n",
    "            doc_vec_norm = [i/total for i in doc_vec]\n",
    "            self.matrix.append(doc_vec_norm)\n",
    "\n",
    "    def inverse_document_freq(self, term):\n",
    "        doc_count = 0\n",
    "        for document in self.corpus:\n",
    "            term_count = self.term_freq(term, document)\n",
    "            if term_count > 0:\n",
    "                doc_count += 1\n",
    "        return math.log( 1.0 * len(self.corpus) / doc_count)\n",
    "\n",
    "def plot_mds(mean_vec):   \n",
    "   from sklearn.manifold import MDS\n",
    "    \n",
    "   data = mean_vec\n",
    "   mds = MDS(n_components=2, random_state=1)\n",
    "   pos = mds.fit_transform(data)\n",
    "   xs,ys = pos[:,0], pos[:,1]\n",
    "   for x, y in zip(xs, ys):\n",
    "       plt.scatter(x, y)\n",
    "    #    plt.text(x, y, name)\n",
    "    #pos2 = mds.fit_transform(model.infer_vector(resume))\n",
    "    #xs2,ys2 = pos2[:,0], pos2[:,1]\n",
    "   plt.scatter(xs[-1], ys[-1], c='Red', marker='+')\n",
    "   plt.text(xs[-1], ys[-1],'resume')\n",
    "   plt.suptitle('MDS')\n",
    "   plt.grid()\n",
    "   plt.savefig('distance_MDS_improved.png')\n",
    "   plt.show()\n",
    "\n",
    "def plot_pca(meav_vec):\n",
    "    from sklearn.decomposition import PCA\n",
    "    data = mean_vec\n",
    "    pca = PCA(n_components=2) #, whiten=True\n",
    "    X = pca.fit_transform(data)\n",
    "    xs,ys =X[:,0], X[:,1]\n",
    "    plt.scatter(X[:,0], X[:,1])\n",
    "    plt.scatter(xs[-1], ys[-1], c='Red', marker='+')\n",
    "    plt.text(xs[-1], ys[-1],'resume')\n",
    "    plt.grid()\n",
    "    plt.suptitle('PCA')\n",
    "    plt.savefig('distance_PCA_improved.png')\n",
    "    plt.show()\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "    level=logging.INFO)\n",
    "\n",
    "#==================================\n",
    "# WARNING: if your computer memory is small, this won't work.\n",
    "#=================================\n",
    "\n",
    "'''    \n",
    "\n",
    "\n",
    "class Rank :\n",
    "\t\"\"\"\n",
    "\tUse this function to determine the appropriate ranking/score of each file.\n",
    "\tWhen instantiated, this class will first load the keywords file\n",
    "\t\n",
    "\t--\n",
    "\t:param list keyword_list\t- list containing each keyword found in keyword_file\n",
    "\t:return Object Rank\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, keyword_list):\n",
    "\t\tself.keywords\t= keyword_list\t\t\n",
    "\t\tself.total_keys = len(self.keywords)\n",
    "\t\n",
    "\tdef get_rank(self, text):\n",
    "\t\t\"\"\"\n",
    "\t\tGet the rank of the file based on total count of keywords found in the file\n",
    "\t\tcontents.\n",
    "\t\t\"\"\"\n",
    "\t\t# set the initial rank and count to 0\n",
    "\t\trank = count = 0\n",
    "\t\t\n",
    "\t\t# get the percentage that each keyword is worth\n",
    "\t\tword_percentage = round(float(100)/float(len(self.keywords)), 2)\n",
    "\t\t# iterate over list of keywords\t\n",
    "\t\tfor keyword in self.keywords :\n",
    "\t\t\tkeyword, multiplier = self.get_multiplier(keyword)\n",
    "\t\t\t\n",
    "\t\t\t# was the keyword found in the file? increase overall percentage if true\n",
    "\t\t\trank += word_percentage if keyword in text else 0\n",
    "\t\t\t\n",
    "\t\t\t# get the number of occurrences of the keyword in the file\n",
    "\t\t\tcount +=  text.count( keyword ) * int( multiplier )\n",
    "\t\t\t\t\t\t\n",
    "\t\tprint \"rank  \",\"Count \", \"done\"\n",
    "\t\treturn (rank,count)\n",
    "\t\t\t\n",
    "\t\n",
    "\tdef get_multiplier(self, keyword):\n",
    "\t\t\"\"\"\n",
    "\t\tSplit the keyword on multiplier delimiter if found. Otherwise provide 1 for multiplier\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tmultiplier \t= 1 \n",
    "\t\t# set the multiplier if found in the file\n",
    "\t\tif ' *' in keyword :\n",
    "\t\t\tkeyword,multiplier = keyword.split(' *')\n",
    "\t\t\n",
    "\t\treturn (keyword, multiplier)\n",
    "\n",
    "def get_keyword(yo):\n",
    "    try:\n",
    "        yo = yo.encode(\"utf-8\")\n",
    "        yo = yo.decode(\"utf-8\")\n",
    "    except:\n",
    "        yo = yo.decode(\"utf-8\")\n",
    "    yo = keywords(yo)\n",
    "    yo =yo.split(\"\\n\")\n",
    "    #ps = PorterStemmer()\n",
    "   # for n,i in enumerate(yo):\n",
    "   #     yo[n] = ps.stem(i)\n",
    "    return yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', \n",
    "    binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 171/171 [00:55<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('ken.txt','r') as f:\n",
    "    resume = f.read()\n",
    "my_resume = get_keyword(resume)\n",
    "\n",
    "bonus = [\"models\", \"modeling risk management\", \"trading\",\"analysis\", 'optimize','financial','models',\\\n",
    "             'development','python','statistics','frm','reinvestment rate','credit','data']\n",
    "\n",
    "for i, word in enumerate(my_resume):\n",
    "    if word in bonus:\n",
    "        my_resume[i] = word + \" *2\"\n",
    "\n",
    "'''\n",
    "with open(\"my_resume_keyword.txt\",\"w\") as file:\n",
    "    for i in my_resume:\n",
    "        i =  i.encode(\"utf-8\")\n",
    "        file.writelines(i)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "df =pd.read_csv('data.csv')\n",
    "jd = df['job_summary'].tolist()\n",
    "companies = df['company'].tolist()\n",
    "positions = df['jobtitle'].tolist()\n",
    "result = pd.DataFrame(columns=[\"company\",\"jobtitle\",\"jd\",\"keywords\",\"rank\",\"count\"])\n",
    "\n",
    "for i in tqdm.tqdm(range(len(jd))):  \n",
    "    j = get_keyword(jd[i])\n",
    "    my_rank = Rank(my_resume)\n",
    "    my_rank = my_rank.get_rank(j)\n",
    "    result = result.append({\"company\":companies[i],\"jobtitle\":positions[i],\"jd\":jd[i],\"keywords\":j,\"rank\":my_rank[0],\"count\":my_rank[1]}, ignore_index=True)\n",
    "    clear_output()\n",
    "result = result.sort_values(by=\"rank\",ascending=False)\n",
    "result = result.reset_index()\n",
    "result = result[:101]\n",
    "result.to_csv(\"job_match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ken.txt','r') as f:\n",
    "    resume = f.read()\n",
    "my_resume = get_keyword(resume)\n",
    "with open(\"my_resume_keyword.txt\",\"w\") as file:\n",
    "    for i in my_resume:\n",
    "        i =  i.encode(\"utf-8\")\n",
    "        file.writelines(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
